{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list1=[]\n",
    "path = \"C:/Users/rohit/Documents/IPO Project/riskfactors\"\n",
    "os.chdir(path)\n",
    "dirs = os.listdir(path)\n",
    "z=0\n",
    "for file in dirs:\n",
    "    ##print os.getcwd()\n",
    "    with open (file, \"r\") as myfile:\n",
    "        ##print myfile\n",
    "        text1=myfile.read()\n",
    "        list1.append(text1)\n",
    "for jaffa in range(0,len(list1)):\n",
    "    list1[jaffa]=nltk.sent_tokenize(list1[jaffa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for daffa in range(0,len(list1)):\n",
    "    for baffa in range(0,(len(list1[daffa]))):\n",
    "        try:\n",
    "            if str(list1[daffa][baffa][0:3]).lower()=='see':\n",
    "                del(list1[daffa][baffa])\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for e in range(0,len(list1)):\n",
    "    list1[e]=[str(e)+' ' + su for su in list1[e]]\n",
    "    for f in range(0,len(list1[e])):\n",
    "        list1[e][f] = str(f)+'_'+ list1[e][f] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"C:/Users/rohit/Documents/IPO Project/boilerplate\"\n",
    "os.chdir(path)\n",
    "sentencelist=pd.read_csv('RF_sentencesonly_70.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(sentencelist)):\n",
    "    sentencelist['sentence'][i]=nltk.sent_tokenize(sentencelist['sentence'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpusdata=[]\n",
    "for i in range(0,len(list1)):\n",
    "    corpusdata.extend(list1[i])\n",
    "    \n",
    "totallength = len(corpusdata)\n",
    "\n",
    "for i in range(0,len(sentencelist)):\n",
    "    corpusdata.extend(sentencelist['sentence'][i])\n",
    "\n",
    "tfidf = TfidfVectorizer().fit_transform(corpusdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"prior market common stock; possible volatility stock price prior offering public market common stock assurance regular trading market common stock develop offering developed sustained.'\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusdata[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561642\n",
      "547262\n",
      "14378\n"
     ]
    }
   ],
   "source": [
    "print len(corpusdata)\n",
    "print totallength\n",
    "print len(sentencelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=('sentence','count','cik'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosine_similarities = linear_kernel(tfidf[547262:547263], tfidf).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547300\n",
      "547400\n",
      "547500\n",
      "547600\n",
      "547700\n",
      "547800\n",
      "547900\n",
      "548000\n",
      "548100\n",
      "548200\n",
      "548300\n",
      "548400\n",
      "548500\n",
      "548600\n",
      "548700\n",
      "548800\n",
      "548900\n",
      "549000\n",
      "549100\n",
      "549200\n",
      "549300\n",
      "549400\n",
      "549500\n",
      "549600\n",
      "549700\n",
      "549800\n",
      "549900\n",
      "550000\n",
      "550100\n",
      "550200\n",
      "550300\n",
      "550400\n",
      "550500\n",
      "550600\n",
      "550700\n",
      "550800\n",
      "550900\n",
      "551000\n",
      "551100\n",
      "551200\n",
      "551300\n",
      "551400\n",
      "551500\n",
      "551600\n",
      "551700\n",
      "551800\n",
      "551900\n",
      "552000\n",
      "552100\n",
      "552200\n",
      "552300\n",
      "552400\n",
      "552500\n",
      "552600\n",
      "552700\n",
      "552800\n",
      "552900\n",
      "553000\n",
      "553100\n",
      "553200\n",
      "553300\n",
      "553400\n",
      "553500\n",
      "553600\n",
      "553700\n",
      "553800\n",
      "553900\n",
      "554000\n",
      "554100\n",
      "554200\n",
      "554300\n",
      "554400\n",
      "554500\n",
      "554600\n",
      "554700\n",
      "554800\n",
      "554900\n",
      "555000\n",
      "555100\n",
      "555200\n",
      "555300\n",
      "555400\n",
      "555500\n",
      "555600\n",
      "555700\n",
      "555800\n",
      "555900\n",
      "556000\n",
      "556100\n",
      "556200\n",
      "556300\n",
      "556400\n",
      "556500\n",
      "556600\n",
      "556700\n",
      "556800\n",
      "556900\n",
      "557000\n",
      "557100\n",
      "557200\n",
      "557300\n",
      "557400\n",
      "557500\n",
      "557600\n",
      "557700\n",
      "557800\n",
      "557900\n",
      "558000\n",
      "558100\n",
      "558200\n",
      "558300\n",
      "558400\n",
      "558500\n",
      "558600\n",
      "558700\n",
      "558800\n",
      "558900\n",
      "559000\n",
      "559100\n",
      "559200\n",
      "559300\n",
      "559400\n",
      "559500\n",
      "559600\n",
      "559700\n",
      "559800\n",
      "559900\n",
      "560000\n",
      "560100\n",
      "560200\n",
      "560300\n",
      "560400\n",
      "560500\n",
      "560600\n",
      "560700\n",
      "560800\n",
      "560900\n",
      "561000\n",
      "561100\n",
      "561200\n",
      "561300\n",
      "561400\n",
      "561500\n",
      "561600\n"
     ]
    }
   ],
   "source": [
    "for tf in range(totallength,len(corpusdata)):\n",
    "    cosine_similarities = linear_kernel(tfidf[tf:tf+1], tfidf[0:totallength]).flatten()\n",
    "    count=0\n",
    "    cik_l=[]\n",
    "    cik_list=[]\n",
    "    related_index=cosine_similarities.tolist()\n",
    "    for v in range(0,len(related_index)):\n",
    "        if related_index[v] >=0.7:\n",
    "            cik_l.append(v)\n",
    "    \n",
    "    for ind1 in cik_l:\n",
    "        cik_list.append(\" \".join(corpusdata[ind1].split()[:1]))\n",
    "            \n",
    "    cik_final_list=str(set(cik_list))[5:-2]\n",
    "    sentence=str(corpusdata[tf:tf+1])[2:-1]\n",
    "    sentence_final=\" \".join(sentence.split()[1:])\n",
    "    df.loc[len(df)+1]=[sentence_final,count,cik_final_list]\n",
    "    if tf%100==0:\n",
    "        print tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>count</th>\n",
       "      <th>cik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>net tangible book value per share.'\"</td>\n",
       "      <td>0</td>\n",
       "      <td>'stock', 'adjusted', 'since', 'investors', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>per share adjusted net tangible book value per...</td>\n",
       "      <td>0</td>\n",
       "      <td>'203_2110', '115_273', '151_1170', '391_1764',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>per share adjusted net tangible book value per...</td>\n",
       "      <td>0</td>\n",
       "      <td>'214_2129', '153_441', '203_2110', '115_273', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>per share net tangible book value per share co...</td>\n",
       "      <td>0</td>\n",
       "      <td>'since', '91_552', 'result', '181_733', 'exten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>per share pro forma net tangible book value sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>'stock', 'adjusted', 'initial', '259_1922', 'i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>net tangible book value per share common stock.'\"</td>\n",
       "      <td>0</td>\n",
       "      <td>'adjusted', 'since', 'investors', '135_940', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>net tangible book value per share.'\"</td>\n",
       "      <td>0</td>\n",
       "      <td>'stock', 'adjusted', 'since', 'investors', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>per share adjusted net tangible book value per...</td>\n",
       "      <td>0</td>\n",
       "      <td>'203_2110', '115_273', '260_1771', 'suffer', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>per share net tangible book value per share co...</td>\n",
       "      <td>0</td>\n",
       "      <td>'approximately', 'investors', 'pro', '203_2110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>per share pro forma net tangible book value pe...</td>\n",
       "      <td>0</td>\n",
       "      <td>'581_2254', '438_1796', '221_2215', 'since', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pro forma net tangible book value per share co...</td>\n",
       "      <td>0</td>\n",
       "      <td>'percent', '263_2217', '231_911', '203_2110', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>prior market common stock; possible volatility...</td>\n",
       "      <td>0</td>\n",
       "      <td>'98_907', '58_883', '151_412', '162_256', 'act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dividends common stock company paid anticipate...</td>\n",
       "      <td>0</td>\n",
       "      <td>'214_406', 'currently', '175_170', 'lack', '24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dividends company anticipate declaring paying ...</td>\n",
       "      <td>0</td>\n",
       "      <td>'110_495', 'currently', 'lack', '87_680', '123...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dividends company anticipate paying cash divid...</td>\n",
       "      <td>0</td>\n",
       "      <td>'193_172', '110_495', 'since', '193_253', 'inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dividends company anticipate paying cash divid...</td>\n",
       "      <td>0</td>\n",
       "      <td>'193_172', '110_495', 'since', '193_253', 'inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dividends company anticipate paying cash divid...</td>\n",
       "      <td>0</td>\n",
       "      <td>'110_495', 'since', '193_253', 'investors', '1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dividends company anticipate paying dividends ...</td>\n",
       "      <td>0</td>\n",
       "      <td>'110_495', 'since', '193_253', '175_170', '134...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dividends company anticipate paying dividends ...</td>\n",
       "      <td>0</td>\n",
       "      <td>'110_495', 'since', '193_253', '175_170', '134...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>dividends company anticipate paying dividends ...</td>\n",
       "      <td>0</td>\n",
       "      <td>'110_495', 'since', '193_253', '175_170', '134...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>dividends company declared paid cash dividends...</td>\n",
       "      <td>0</td>\n",
       "      <td>'declared', 'currently', '157_657', 'investors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>dividends company never declared paid dividend...</td>\n",
       "      <td>0</td>\n",
       "      <td>'121_628', '261_1025', 'declared', '193_253', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>dividends company never declared paid dividend...</td>\n",
       "      <td>0</td>\n",
       "      <td>'214_406', 'currently', '301_1879', '175_170',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>dividends company never paid cash dividends co...</td>\n",
       "      <td>0</td>\n",
       "      <td>'214_406', 'currently', '73_545', '485_1856', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>dividends company never paid cash dividends co...</td>\n",
       "      <td>0</td>\n",
       "      <td>'investors', 'payment', 'lack', '87_680', '142...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>dividends company never paid cash dividends co...</td>\n",
       "      <td>0</td>\n",
       "      <td>'214_406', 'currently', '73_545', '485_1856', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>dividends company never paid cash dividends co...</td>\n",
       "      <td>0</td>\n",
       "      <td>'214_406', 'currently', '73_545', '485_1856', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>dividends company never paid cash dividends co...</td>\n",
       "      <td>0</td>\n",
       "      <td>'214_406', 'currently', '73_545', '485_1856', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>dividends company never paid cash dividends co...</td>\n",
       "      <td>0</td>\n",
       "      <td>'214_406', 'currently', '73_545', '485_1856', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>dividends company never paid cash dividends co...</td>\n",
       "      <td>0</td>\n",
       "      <td>'214_406', '57_956', '239_632', 'investors', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14351</th>\n",
       "      <td>believe may estimate continue anticipate inten...</td>\n",
       "      <td>0</td>\n",
       "      <td>'512_1911', '104_838', '2_1211', '2_1210', '32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14352</th>\n",
       "      <td>believe may estimate continue anticipate inten...</td>\n",
       "      <td>0</td>\n",
       "      <td>'512_1911', '104_838', '2_1211', '2_1210', '32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14353</th>\n",
       "      <td>believe may estimate continue anticipate inten...</td>\n",
       "      <td>0</td>\n",
       "      <td>'512_1911', '104_838', '2_1211', '2_1210', '32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14354</th>\n",
       "      <td>believe may estimate continue anticipate inten...</td>\n",
       "      <td>0</td>\n",
       "      <td>'512_1911', '104_838', '2_1211', '2_1210', '32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14355</th>\n",
       "      <td>believe may estimate continue anticipate inten...</td>\n",
       "      <td>0</td>\n",
       "      <td>'320_1621', 'used', '255_34', 'particular', 't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14356</th>\n",
       "      <td>believe may estimate continue anticipate inten...</td>\n",
       "      <td>0</td>\n",
       "      <td>'183_1626', '313_1845', '320_1621', '239_2047'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14357</th>\n",
       "      <td>believe may estimate continue anticipate inten...</td>\n",
       "      <td>0</td>\n",
       "      <td>'183_1626', '313_1845', '320_1621', '239_2047'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14358</th>\n",
       "      <td>believe may estimate continue anticipate inten...</td>\n",
       "      <td>0</td>\n",
       "      <td>'183_1626', '320_1621', '210_2063', '307_1994'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14359</th>\n",
       "      <td>believe may estimate continue anticipate inten...</td>\n",
       "      <td>0</td>\n",
       "      <td>'2_1211', '2_1210', '335_1682', '3_1139', '211...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14360</th>\n",
       "      <td>believe may estimate continue anticipate inten...</td>\n",
       "      <td>0</td>\n",
       "      <td>'2_1211', '2_1210', '335_1682', '3_1139', '211...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14361</th>\n",
       "      <td>believe may estimate continue anticipate inten...</td>\n",
       "      <td>0</td>\n",
       "      <td>'183_1626', '313_1845', '320_1621', '239_2047'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14362</th>\n",
       "      <td>believes anticipates plans expects intends est...</td>\n",
       "      <td>0</td>\n",
       "      <td>'280_235', '188_1132', '207_212', '315_1979', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14363</th>\n",
       "      <td>believes intends expects anticipates plans sim...</td>\n",
       "      <td>0</td>\n",
       "      <td>'280_235', '188_1132', '251_1262', '207_212', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14364</th>\n",
       "      <td>expect anticipate intend plan believe seek est...</td>\n",
       "      <td>0</td>\n",
       "      <td>'244_1125', '183_1626', '283_1784', 'often', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14365</th>\n",
       "      <td>expect intend plan believe project anticipate ...</td>\n",
       "      <td>0</td>\n",
       "      <td>'244_1125', '183_1626', '320_1621', '223_676',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14366</th>\n",
       "      <td>expect intend plan believe project may estimat...</td>\n",
       "      <td>0</td>\n",
       "      <td>'244_1125', '183_1626', 'words', '210_2063', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14367</th>\n",
       "      <td>expects anticipates estimates intends believes...</td>\n",
       "      <td>0</td>\n",
       "      <td>'280_235', '207_212', '315_1979', '54_233', '1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14368</th>\n",
       "      <td>expects intends plans projects believes estima...</td>\n",
       "      <td>0</td>\n",
       "      <td>'280_235', 'statements', 'words', '332_2197', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14369</th>\n",
       "      <td>including may could would anticipates expects ...</td>\n",
       "      <td>0</td>\n",
       "      <td>'314_1677', '280_235', 'statements', 'words', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14370</th>\n",
       "      <td>may could would expect intend plan anticipate ...</td>\n",
       "      <td>0</td>\n",
       "      <td>'559_2228', '576_2138', '223_676', 'often', '1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14371</th>\n",
       "      <td>may could would predicts potential continue ex...</td>\n",
       "      <td>0</td>\n",
       "      <td>'433_1587', '309_1674', 'use', 'used', '364_19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14372</th>\n",
       "      <td>may could would predicts potential continue ex...</td>\n",
       "      <td>0</td>\n",
       "      <td>'433_1587', '309_1674', 'use', 'used', '364_19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14373</th>\n",
       "      <td>may could would predicts potential continue ex...</td>\n",
       "      <td>0</td>\n",
       "      <td>'433_1587', '309_1674', 'use', 'used', '364_19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14374</th>\n",
       "      <td>ended december compared year ended december re...</td>\n",
       "      <td>0</td>\n",
       "      <td>'limited', 'losses', 'interest', 'million', '1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14375</th>\n",
       "      <td>ended december compared year ended december re...</td>\n",
       "      <td>0</td>\n",
       "      <td>'net', 'million', 'example', 'year'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14376</th>\n",
       "      <td>ended december compared year ended december re...</td>\n",
       "      <td>0</td>\n",
       "      <td>'net', 'million', 'example', 'year'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14377</th>\n",
       "      <td>ended december nine months ended september net...</td>\n",
       "      <td>0</td>\n",
       "      <td>'sales', 'pro', 'product', '17_1122', 'years',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14378</th>\n",
       "      <td>ended december six months ended june net loss ...</td>\n",
       "      <td>0</td>\n",
       "      <td>'7_1426', '3_1405', 'consolidated', '17_1122',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14379</th>\n",
       "      <td>ended december six months ended june reported ...</td>\n",
       "      <td>0</td>\n",
       "      <td>'3_1405', '17_1122', 'expenses', 'cost', 'incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14380</th>\n",
       "      <td>ended six months ended june spent $.million $....</td>\n",
       "      <td>0</td>\n",
       "      <td>'product', 'experienced', 'years', 'million', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14380 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  count  \\\n",
       "1                   net tangible book value per share.'\"      0   \n",
       "2      per share adjusted net tangible book value per...      0   \n",
       "3      per share adjusted net tangible book value per...      0   \n",
       "4      per share net tangible book value per share co...      0   \n",
       "5      per share pro forma net tangible book value sh...      0   \n",
       "6      net tangible book value per share common stock.'\"      0   \n",
       "7                   net tangible book value per share.'\"      0   \n",
       "8      per share adjusted net tangible book value per...      0   \n",
       "9      per share net tangible book value per share co...      0   \n",
       "10     per share pro forma net tangible book value pe...      0   \n",
       "11     pro forma net tangible book value per share co...      0   \n",
       "12     prior market common stock; possible volatility...      0   \n",
       "13     dividends common stock company paid anticipate...      0   \n",
       "14     dividends company anticipate declaring paying ...      0   \n",
       "15     dividends company anticipate paying cash divid...      0   \n",
       "16     dividends company anticipate paying cash divid...      0   \n",
       "17     dividends company anticipate paying cash divid...      0   \n",
       "18     dividends company anticipate paying dividends ...      0   \n",
       "19     dividends company anticipate paying dividends ...      0   \n",
       "20     dividends company anticipate paying dividends ...      0   \n",
       "21     dividends company declared paid cash dividends...      0   \n",
       "22     dividends company never declared paid dividend...      0   \n",
       "23     dividends company never declared paid dividend...      0   \n",
       "24     dividends company never paid cash dividends co...      0   \n",
       "25     dividends company never paid cash dividends co...      0   \n",
       "26     dividends company never paid cash dividends co...      0   \n",
       "27     dividends company never paid cash dividends co...      0   \n",
       "28     dividends company never paid cash dividends co...      0   \n",
       "29     dividends company never paid cash dividends co...      0   \n",
       "30     dividends company never paid cash dividends co...      0   \n",
       "...                                                  ...    ...   \n",
       "14351  believe may estimate continue anticipate inten...      0   \n",
       "14352  believe may estimate continue anticipate inten...      0   \n",
       "14353  believe may estimate continue anticipate inten...      0   \n",
       "14354  believe may estimate continue anticipate inten...      0   \n",
       "14355  believe may estimate continue anticipate inten...      0   \n",
       "14356  believe may estimate continue anticipate inten...      0   \n",
       "14357  believe may estimate continue anticipate inten...      0   \n",
       "14358  believe may estimate continue anticipate inten...      0   \n",
       "14359  believe may estimate continue anticipate inten...      0   \n",
       "14360  believe may estimate continue anticipate inten...      0   \n",
       "14361  believe may estimate continue anticipate inten...      0   \n",
       "14362  believes anticipates plans expects intends est...      0   \n",
       "14363  believes intends expects anticipates plans sim...      0   \n",
       "14364  expect anticipate intend plan believe seek est...      0   \n",
       "14365  expect intend plan believe project anticipate ...      0   \n",
       "14366  expect intend plan believe project may estimat...      0   \n",
       "14367  expects anticipates estimates intends believes...      0   \n",
       "14368  expects intends plans projects believes estima...      0   \n",
       "14369  including may could would anticipates expects ...      0   \n",
       "14370  may could would expect intend plan anticipate ...      0   \n",
       "14371  may could would predicts potential continue ex...      0   \n",
       "14372  may could would predicts potential continue ex...      0   \n",
       "14373  may could would predicts potential continue ex...      0   \n",
       "14374  ended december compared year ended december re...      0   \n",
       "14375  ended december compared year ended december re...      0   \n",
       "14376  ended december compared year ended december re...      0   \n",
       "14377  ended december nine months ended september net...      0   \n",
       "14378  ended december six months ended june net loss ...      0   \n",
       "14379  ended december six months ended june reported ...      0   \n",
       "14380  ended six months ended june spent $.million $....      0   \n",
       "\n",
       "                                                     cik  \n",
       "1      'stock', 'adjusted', 'since', 'investors', 'pr...  \n",
       "2      '203_2110', '115_273', '151_1170', '391_1764',...  \n",
       "3      '214_2129', '153_441', '203_2110', '115_273', ...  \n",
       "4      'since', '91_552', 'result', '181_733', 'exten...  \n",
       "5      'stock', 'adjusted', 'initial', '259_1922', 'i...  \n",
       "6      'adjusted', 'since', 'investors', '135_940', '...  \n",
       "7      'stock', 'adjusted', 'since', 'investors', 'pr...  \n",
       "8      '203_2110', '115_273', '260_1771', 'suffer', '...  \n",
       "9      'approximately', 'investors', 'pro', '203_2110...  \n",
       "10     '581_2254', '438_1796', '221_2215', 'since', '...  \n",
       "11     'percent', '263_2217', '231_911', '203_2110', ...  \n",
       "12     '98_907', '58_883', '151_412', '162_256', 'act...  \n",
       "13     '214_406', 'currently', '175_170', 'lack', '24...  \n",
       "14     '110_495', 'currently', 'lack', '87_680', '123...  \n",
       "15     '193_172', '110_495', 'since', '193_253', 'inv...  \n",
       "16     '193_172', '110_495', 'since', '193_253', 'inv...  \n",
       "17     '110_495', 'since', '193_253', 'investors', '1...  \n",
       "18     '110_495', 'since', '193_253', '175_170', '134...  \n",
       "19     '110_495', 'since', '193_253', '175_170', '134...  \n",
       "20     '110_495', 'since', '193_253', '175_170', '134...  \n",
       "21     'declared', 'currently', '157_657', 'investors...  \n",
       "22     '121_628', '261_1025', 'declared', '193_253', ...  \n",
       "23     '214_406', 'currently', '301_1879', '175_170',...  \n",
       "24     '214_406', 'currently', '73_545', '485_1856', ...  \n",
       "25     'investors', 'payment', 'lack', '87_680', '142...  \n",
       "26     '214_406', 'currently', '73_545', '485_1856', ...  \n",
       "27     '214_406', 'currently', '73_545', '485_1856', ...  \n",
       "28     '214_406', 'currently', '73_545', '485_1856', ...  \n",
       "29     '214_406', 'currently', '73_545', '485_1856', ...  \n",
       "30     '214_406', '57_956', '239_632', 'investors', '...  \n",
       "...                                                  ...  \n",
       "14351  '512_1911', '104_838', '2_1211', '2_1210', '32...  \n",
       "14352  '512_1911', '104_838', '2_1211', '2_1210', '32...  \n",
       "14353  '512_1911', '104_838', '2_1211', '2_1210', '32...  \n",
       "14354  '512_1911', '104_838', '2_1211', '2_1210', '32...  \n",
       "14355  '320_1621', 'used', '255_34', 'particular', 't...  \n",
       "14356  '183_1626', '313_1845', '320_1621', '239_2047'...  \n",
       "14357  '183_1626', '313_1845', '320_1621', '239_2047'...  \n",
       "14358  '183_1626', '320_1621', '210_2063', '307_1994'...  \n",
       "14359  '2_1211', '2_1210', '335_1682', '3_1139', '211...  \n",
       "14360  '2_1211', '2_1210', '335_1682', '3_1139', '211...  \n",
       "14361  '183_1626', '313_1845', '320_1621', '239_2047'...  \n",
       "14362  '280_235', '188_1132', '207_212', '315_1979', ...  \n",
       "14363  '280_235', '188_1132', '251_1262', '207_212', ...  \n",
       "14364  '244_1125', '183_1626', '283_1784', 'often', '...  \n",
       "14365  '244_1125', '183_1626', '320_1621', '223_676',...  \n",
       "14366  '244_1125', '183_1626', 'words', '210_2063', '...  \n",
       "14367  '280_235', '207_212', '315_1979', '54_233', '1...  \n",
       "14368  '280_235', 'statements', 'words', '332_2197', ...  \n",
       "14369  '314_1677', '280_235', 'statements', 'words', ...  \n",
       "14370  '559_2228', '576_2138', '223_676', 'often', '1...  \n",
       "14371  '433_1587', '309_1674', 'use', 'used', '364_19...  \n",
       "14372  '433_1587', '309_1674', 'use', 'used', '364_19...  \n",
       "14373  '433_1587', '309_1674', 'use', 'used', '364_19...  \n",
       "14374  'limited', 'losses', 'interest', 'million', '1...  \n",
       "14375                'net', 'million', 'example', 'year'  \n",
       "14376                'net', 'million', 'example', 'year'  \n",
       "14377  'sales', 'pro', 'product', '17_1122', 'years',...  \n",
       "14378  '7_1426', '3_1405', 'consolidated', '17_1122',...  \n",
       "14379  '3_1405', '17_1122', 'expenses', 'cost', 'incl...  \n",
       "14380  'product', 'experienced', 'years', 'million', ...  \n",
       "\n",
       "[14380 rows x 3 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"% net tangible book value per share.'\""
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusdata[547262]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x604147 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[547262:547263]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosine_similarities = linear_kernel(tfidf[547262:547263], tfidf[0:547262]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77_367 This represents immediate increase net tangible book value $1.10 per share existing shareholders immediate net tangible book value dilution $5.32 per share purchasers Offering.\n",
      "103_382 Upon completion Offering, net tangible book value per share Common Stock, would $.78, representing immediate dilution $4.22 net tangible book value per share, 84.4%, public investors increase $.86 per share existing shareholders.\n",
      "243_574 This dilution reduce net tangible book value shares since shares common stock purchase offering substantially higher per share price current average net tangible book value per share common stock.\n",
      "85_838 The purchasers Common Stock offered hereby experience immediate significant dilution net tangible book value per share, present shareholders experience material increase net tangible book value per share.\n",
      "303_1148 24 DILUTION Dilution net tangible book value per share represents difference amount per share paid purchasers common stock offering net tangible book value per share common stock immediately offering.\n",
      "283_1215 Because expect offering price substantially higher net tangible book value per share common stock, purchase shares offering, incur dilution net tangible book value per share shares based assumed initial public offering price per share.\n",
      "291_1571 Dilution net tangible book value per share represents difference amount per share paid purchasers shares common stock offering net tangible book value per share common stock immediately completion offering.\n",
      "183_2010 If purchase shares common stock offering initial public offering price $13.50 per share, experience immediate substantial dilution $9.39 pro forma net tangible book value per share price pay substantially greater net tangible book value per share shares acquire, based net tangible book value per share October2, 2004.\n",
      "207_2124 Investors purchasing shares offering incur immediate substantial dilution net tangible book value per share price new investors pay substantially greater net tangible book value per share shares acquired.\n",
      "24_2150 The following table illustrates per share dilution Initial public offering price per share 11.00 Pro forma net tangible book value per share offering $1.32 Increase pro forma net tangible book value per share attributable new investors 2.14 Pro forma adjusted net tangible book value per share offering 3.46 Dilution per share new investors 7.54 If underwriters exercise overallotment option full, pro forma adjusted net tangible book value per share offering increase approximately $3.71 per share, representing increase existing stockholders approximately $2.38 per share, immediate dilution approximately $7.29 per share new investors.\n",
      "451_2151 Based net tangible book value (deficiency) per share $(3.33) June 30, 2006 initial public offering price $12.00 per share, investors purchasing common stock offering incur immediate dilution approximately $11.38 per share net tangible book value per share common stock.\n",
      "368_2171 Since price per share common stock offered substantially higher net tangible book value per share common stock, suffer substantial dilution net tangible book value common stock purchase offering.\n",
      "423_2185 Dilution net tangible book value per share represents difference amount per share paid purchasers common stock offering net tangible book value per share common stock immediately thereafter.\n",
      "426_2185 The following table illustrates per share dilution Assumed initial public offering price per share common stock 13.00 Net tangible book value per share common stock prior offering formation transactions(1) (0.52 Net increase pro forma net tangible book value per share common stock attributable formation transactions offering (2) 6.80 Pro forma net tangible book value per share common stock offering formation transactions (3) 6.28 Dilution pro forma net tangible book value per share common stock new investors (4) 6.72 (1) Historical net tangible book value per share determined dividing net tangible assets September30, 2006 (net book value tangible assets consists total assets less intangible assets, consist deferred financing costs acquired inplace lease value net liabilities assumed) predecessor number shares common stock outstanding, assuming issuance shares common stock issuable formation transactions (other contingent issuances) offering.\n",
      "299_2233 Investors purchasing shares offering incur immediate substantial dilution net tangible book value per share price new investors pay substantially greater net tangible book value per share shares acquired.\n",
      "'451_2151', '423_2185', '24_2150', '85_838', '183_2010', '243_574', '426_2185', '291_1571', '299_2233', '207_2124', '283_1215', '303_1148', '77_367', '103_382', '368_2171'\n"
     ]
    }
   ],
   "source": [
    "related_index=cosine_similarities.tolist()\n",
    "count=0\n",
    "cik_l=[]\n",
    "cik_list=[]\n",
    "for v in range(0,len(related_index)):\n",
    "    if related_index[v] >=0.7:\n",
    "        cik_l.append(v)\n",
    "    \n",
    "for ind1 in cik_l:\n",
    "    cik_list.append(\" \".join(corpusdata[ind1].split()[:1]))\n",
    "    print corpusdata[ind1]\n",
    "cik_final_list=str(set(cik_list))[5:-2]\n",
    "print cik_final_list\n",
    "#sentence=str(corpusdata[tf:tf+1])[2:-1]\n",
    "#sentence_final=\" \".join(sentence.split()[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Below code for aggregating all the above and below sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('RF_sentence_placing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bf6e61d441e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:/Users/rohit/Documents/IPO Project/boilerplate\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RF_sentencesonly_70.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/rohit/Documents/IPO Project/boilerplate\"\n",
    "os.chdir(path)\n",
    "myfile=pd.read_csv('RF_sentencesonly_70.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['154_4']\n",
      "['155_4']\n",
      "['180_4']\n",
      "['181_4']\n",
      "['185_4']\n",
      "['190_4']\n",
      "['81_11']\n",
      "['92_11']\n",
      "['361_16']\n",
      "['362_16']\n",
      "['363_16']\n",
      "['379_16']\n",
      "['382_16']\n",
      "['136_18']\n",
      "['140_18']\n",
      "['181_22']\n",
      "['183_22']\n",
      "['184_22']\n",
      "['186_22']\n",
      "['187_22']\n",
      "['133_23']\n",
      "['145_23']\n",
      "['157_26']\n",
      "['162_26']\n",
      "['170_26']\n",
      "['171_26']\n",
      "['174_26']\n",
      "['178_26']\n",
      "['144_27']\n",
      "['163_28']\n",
      "['180_31']\n",
      "['182_31']\n",
      "['188_31']\n",
      "['200_33']\n",
      "['201_33']\n",
      "['207_33']\n",
      "['215_33']\n",
      "['222_33']\n",
      "['225_34']\n",
      "['229_34']\n",
      "['230_34']\n",
      "['231_34']\n",
      "['244_34']\n",
      "['245_34']\n",
      "['251_34']\n",
      "['115_37']\n",
      "['137_37']\n",
      "['138_37']\n",
      "['246_45']\n",
      "['248_45']\n",
      "['249_45']\n",
      "['273_45']\n",
      "['287_45']\n",
      "['187_49']\n",
      "['190_49']\n",
      "['192_49']\n",
      "['202_49']\n",
      "['100_52']\n",
      "['160_54']\n",
      "['108_55']\n",
      "['112_55']\n",
      "['114_55']\n",
      "['115_55']\n",
      "['116_55']\n",
      "['117_55']\n",
      "['287_56']\n",
      "['288_56']\n",
      "['290_56']\n",
      "['311_56']\n",
      "['121_65']\n",
      "['207_70']\n",
      "['218_70']\n",
      "['219_70']\n",
      "['229_70']\n",
      "['230_70']\n",
      "['128_74']\n",
      "['130_74']\n",
      "['134_74']\n",
      "['148_74']\n",
      "['166_75']\n",
      "['173_75']\n",
      "['186_75']\n",
      "['189_75']\n",
      "['183_77']\n",
      "['184_77']\n",
      "['195_77']\n",
      "['156_82']\n",
      "['159_82']\n",
      "['3_87']\n",
      "['195_87']\n",
      "['196_87']\n",
      "['197_87']\n",
      "['198_87']\n",
      "['219_87']\n",
      "['55_90']\n",
      "['60_90']\n",
      "['78_90']\n",
      "['81_90']\n",
      "['82_90']\n",
      "['251_93']\n",
      "['252_93']\n",
      "['284_93']\n",
      "['285_93']\n",
      "['283_95']\n",
      "['284_95']\n",
      "['286_95']\n",
      "['324_95']\n",
      "['343_95']\n",
      "['438_96']\n",
      "['440_96']\n",
      "['517_96']\n",
      "['530_96']\n",
      "['531_96']\n",
      "['567_96']\n",
      "['221_101']\n",
      "['222_101']\n",
      "['237_101']\n",
      "['238_101']\n",
      "['244_101']\n",
      "['130_103']\n",
      "['133_103']\n",
      "['134_103']\n",
      "['135_103']\n",
      "['137_103']\n",
      "['318_107']\n",
      "['319_107']\n",
      "['321_107']\n",
      "['345_107']\n",
      "['347_107']\n",
      "['348_107']\n",
      "['213_112']\n",
      "['206_116']\n",
      "['222_116']\n",
      "['223_116']\n",
      "['224_116']\n",
      "['238_116']\n",
      "['239_116']\n",
      "['254_117']\n",
      "['256_117']\n",
      "['257_117']\n",
      "['262_117']\n",
      "['230_119']\n",
      "['386_119']\n",
      "['387_119']\n",
      "['388_119']\n",
      "['408_119']\n",
      "['147_121']\n",
      "['149_121']\n",
      "['155_121']\n",
      "['167_121']\n",
      "['129_126']\n",
      "['133_126']\n",
      "['137_126']\n",
      "['138_126']\n",
      "['182_127']\n",
      "['255_128']\n",
      "['260_128']\n",
      "['109_131']\n",
      "['218_132']\n",
      "['219_132']\n",
      "['224_132']\n",
      "['229_132']\n",
      "['203_134']\n",
      "['250_135']\n",
      "['257_135']\n",
      "['241_136']\n",
      "['243_136']\n",
      "['262_136']\n",
      "['263_136']\n",
      "['263_144']\n",
      "['342_151']\n",
      "['375_151']\n",
      "['380_151']\n",
      "['381_151']\n",
      "['382_151']\n",
      "['194_152']\n",
      "['197_152']\n",
      "['198_152']\n",
      "['199_152']\n",
      "['200_152']\n",
      "['202_152']\n",
      "['203_152']\n",
      "['281_155']\n",
      "['289_155']\n",
      "['290_155']\n",
      "['291_155']\n",
      "['248_156']\n",
      "['251_156']\n",
      "['252_156']\n",
      "['198_157']\n",
      "['199_157']\n",
      "['203_157']\n",
      "['214_157']\n",
      "['166_162']\n",
      "['167_162']\n",
      "['170_162']\n",
      "['170_164']\n",
      "['172_164']\n",
      "['178_164']\n",
      "['209_164']\n",
      "['246_184']\n",
      "['247_184']\n",
      "['249_184']\n",
      "['256_184']\n",
      "['119_189']\n",
      "['126_189']\n",
      "['134_189']\n",
      "['135_189']\n",
      "['142_189']\n",
      "['146_189']\n",
      "['143_192']\n",
      "['170_212']\n",
      "['176_212']\n",
      "['186_212']\n",
      "['189_212']\n",
      "['190_212']\n",
      "['145_213']\n",
      "['136_219']\n",
      "['137_219']\n",
      "['143_219']\n",
      "['144_219']\n",
      "['145_219']\n",
      "['172_226']\n",
      "['174_226']\n",
      "['176_226']\n",
      "['189_226']\n",
      "['125_227']\n",
      "['126_227']\n",
      "['143_227']\n",
      "['207_230']\n",
      "['221_230']\n",
      "['225_230']\n",
      "['194_235']\n",
      "['195_235']\n",
      "['216_235']\n",
      "['179_237']\n",
      "['181_237']\n",
      "['101_239']\n",
      "['114_239']\n",
      "['188_242']\n",
      "['202_242']\n",
      "['203_242']\n",
      "['211_242']\n",
      "['189_254']\n",
      "['196_254']\n",
      "['197_254']\n",
      "['201_257']\n",
      "['204_257']\n",
      "['41_258']\n",
      "['176_264']\n",
      "['183_264']\n",
      "['192_264']\n",
      "['198_264']\n",
      "['210_264']\n",
      "['3_267']\n",
      "['375_276']\n",
      "['376_276']\n",
      "['378_276']\n",
      "['380_276']\n",
      "['3_279']\n",
      "['207_279']\n",
      "['221_279']\n",
      "['558_290']\n",
      "['559_290']\n",
      "['560_290']\n",
      "['561_290']\n",
      "['562_290']\n",
      "['563_290']\n",
      "['558_291']\n",
      "['559_291']\n",
      "['560_291']\n",
      "['561_291']\n",
      "['562_291']\n",
      "['563_291']\n",
      "['558_292']\n",
      "['559_292']\n",
      "['560_292']\n",
      "['561_292']\n",
      "['562_292']\n",
      "['563_292']\n",
      "['558_293']\n",
      "['559_293']\n",
      "['560_293']\n",
      "['561_293']\n",
      "['562_293']\n",
      "['563_293']\n",
      "['3_296']\n",
      "['257_296']\n",
      "['258_296']\n",
      "['260_296']\n",
      "['238_297']\n",
      "['253_297']\n",
      "['138_301']\n",
      "['139_301']\n",
      "['144_301']\n",
      "['148_301']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-593e597550b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mtext2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mvector1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_to_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mvector2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_to_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mcosine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cosine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-104-27a0724e9a6c>\u001b[0m in \u001b[0;36mtext_to_vector\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtext_to_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m      \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWORD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m      \u001b[1;32mreturn\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\rohit\\Anaconda\\lib\\collections.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    463\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rohit\\Anaconda\\lib\\collections.pyc\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m         \u001b[0miterable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0miterable\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(0,len(myfile)):\n",
    "    \n",
    "    print i\n",
    "    text1 = myfile['sentence'][i]\n",
    "    full_cik=''\n",
    "    for j in range(0,len(list1)):\n",
    "        for k in range(0,len(list1[j])):\n",
    "            text2 = list1[j][k]\n",
    "            vector1 = text_to_vector(text1)\n",
    "            vector2 = text_to_vector(text2)\n",
    "            cosine = get_cosine(vector1, vector2)\n",
    "            if cosine>=0.50:\n",
    "                print text2.split()[:1]\n",
    "                #myfile['count'][j]=0\n",
    "                #myfile['sentence'][j]=''\n",
    "            #new_cik=new_cik+' ,'+str(myfile['cik'][j])\n",
    "        #full_cik=full_cik+new_cik\n",
    "    #myfile['cik'][i]=str(set(full_cik.split()))[5:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"prior market common stock; possible volatility stock price prior offering public market common stock assurance regular trading market common stock develop offering developed sustained.'\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile['sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newfile=pd.read_csv('riskfact_sentence_numbering.csv')\n",
    "df=pd.DataFrame(columns=('index','origin_sentence','sentence_num','cik','equivalent_sentence','before_sentence','after_sentence','before4','after4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(newfile)):\n",
    "    print i\n",
    "    sentnum=str(newfile['cik'][i])[:-1]\n",
    "    sentlist=sentnum.split(',')\n",
    "    for j in range(0,len(sentlist)):\n",
    "        #print j\n",
    "        temp=sentlist[j]\n",
    "        snum= temp.split('_')[0]\n",
    "        docnum= temp.split('_')[1]\n",
    "        bnum=int(snum)-1\n",
    "        anum=int(snum)+1\n",
    "        b4num=int(snum)-4\n",
    "        a4num=int(snum)+4\n",
    "        try:\n",
    "            beforesent = list1[int(docnum)][int(bnum)]\n",
    "        except:\n",
    "            beforesent = 'start of sentence'\n",
    "        try:\n",
    "            aftersent = list1[int(docnum)][anum]\n",
    "        except:\n",
    "            aftersent = 'end of sentence'\n",
    "        try:\n",
    "            equivalent_sent = list1[int(docnum)][int(snum)]\n",
    "        except:\n",
    "            equivalent_sent='equivalent sentence cant be found'\n",
    "        try:\n",
    "            beforesent4 = list1[int(docnum)][b4num:int(snum)]\n",
    "        except:\n",
    "            beforesent4 = 'start of sentence'\n",
    "        try:\n",
    "            aftersent4 = list1[int(docnum)][int(snum):a4num]\n",
    "        except:\n",
    "            aftersent4 = 'end of sentence'\n",
    "        df.loc[len(df)+1]=[i,newfile['sentence'][i],snum,docnum,equivalent_sent,beforesent,aftersent,beforesent4,aftersent4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11\n",
      "8_569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8'"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \n",
    "print j\n",
    "print temp\n",
    "temp.split('_')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('similarsentence_data_full_riskfact_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8_569 We incurred net losses $4.7 million $6.6 million years ended December 31, 1997 1998, respectively, well net loss $6.1 million six months ended June 30, 1999.', '9_569 As June 30, 1999, incurred cumulative net losses Ccorporation $18.2 million.', '10_569 WE ANTICIPATE CONTINUED LOSSES Although believe success depend large part upon ability generate sufficient revenue achieve profitability effectively maintain existing relationships develop new relationships customers strategic partners, revenue may increase, may achieve maintain profitability.', '11_569 In particular, intend expend significant financial management resources product development, sales marketing, strategic relationships technology, operating infrastructure.']\n"
     ]
    }
   ],
   "source": [
    "print list1[569][8:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
