{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named bootstrapping",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6af956362a69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbootstrapping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBootstrapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequentialTagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhp_classifiers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHpObj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHpSubj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named bootstrapping"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import pickle\n",
    "from bootstrapping import Bootstrapping\n",
    "from pos import SequentialTagger\n",
    "from hp_classifiers import HpObj, HpSubj\n",
    "from polarity import PolarityClassifier  \n",
    "from replacer import RepeatReplacer\n",
    "from terminal_colors import Tcolors\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "class Sentiment:\n",
    "    \"\"\"\n",
    "        Sentiment: Analyses the global sentiment of given text regions  \n",
    "        that are decomposed to sentences, using bootstrapping methods for \n",
    "        subjectivity and polarity classification. All sub modules except \n",
    "        from POS tagging are learning by experience.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pos_tagger = SequentialTagger()\n",
    "        self.hp_obj = HpObj(debug=DEBUG)\n",
    "        self.hp_subj = HpSubj(debug=DEBUG)\n",
    "        self.lexicon = self.hp_obj.lexicon\n",
    "        self.bootstrapping = Bootstrapping(self.hp_obj, self.hp_subj, self.pos_tagger, debug=DEBUG) \n",
    "        self.sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.total_sentences = [\"good\",\"bad\"]\n",
    "        self.total_sentiments = [\"positive\",\"negative\"]\n",
    "        \n",
    "    def analyze(self, clean_text_areas):\n",
    "        \"\"\"\n",
    "            Analysis of text regions using the following order: Each sentence per\n",
    "            region is passed from the subjectivity classification using bootstrapping\n",
    "            method and then if it turns out to be subjective it is passed \n",
    "            from the polarity classification using bootstrapping method also.\n",
    "            Finally, it results to a decision for the sentiment of the sentence\n",
    "            and the overall sentiment of the regions. \n",
    "        \"\"\" \n",
    "        if len(clean_text_areas) > 0:  \n",
    "            for clean_text in clean_text_areas:\n",
    "                # Sentence detection\n",
    "                clean_text = self.normalize(clean_text)\n",
    "                try:\n",
    "                    sentences = self.sentence_tokenizer.tokenize(clean_text)\n",
    "                except:\n",
    "                    return {}\n",
    "                sentiments = [] \n",
    "                scores = []\n",
    "                nscores = []\n",
    "                results = {'positive':{'count' : 0, 'score' : 0, 'nscore' : 0},\n",
    "                           'neutral':{'count' : 0, 'score' : 0, 'nscore' : 0},\n",
    "                           'negative':{'count' : 0, 'score' : 0, 'nscore' : 0}}\n",
    "                \n",
    "                print\n",
    "                print Tcolors.ACT + \" Checking block of text:\"\n",
    "                for i, sentence in enumerate(sentences):\n",
    "                    print \"[\" + str(i+1) + \"] \" + sentence\n",
    "                for i, sentence in enumerate(sentences):\n",
    "                    # Proceed to subjectivity classification (bootstrapping procedure).\n",
    "                    # (This step could be skipped in case you deal with subjective sentences only.)\n",
    "                    sentiment = \"\"\n",
    "                    previous = \"\"\n",
    "                    next = \"\"\n",
    "                    score = 0\n",
    "                    nscore = 0\n",
    "                    if i == 0 and i + 1 < len(sentences): \n",
    "                        next = sentences[i+1] \n",
    "                    elif i != 0 and i < len(sentences):\n",
    "                        if i + 1 != len(sentences):\n",
    "                            next = sentences[i+1]\n",
    "                        previous = sentences[i-1] \n",
    "                     \n",
    "                    if DEBUG: print Tcolors.ACT + \" Analyzing subjectivity...\" \n",
    "                    result = self.bootstrapping.classify(sentence, previous, next) \n",
    "                    if result is None:\n",
    "                        res = 'Not found!'\n",
    "                    else:\n",
    "                        res = result\n",
    "                    if DEBUG:\n",
    "                        print Tcolors.RES + Tcolors.OKGREEN + \" \" + res + Tcolors.ENDC\n",
    "                        print\n",
    "                    \n",
    "                    # If sentence is subjective \n",
    "                    if result == 'subjective' or result is None:\n",
    "                        # Proceed to polarity classification\n",
    "                        if DEBUG: print Tcolors.ACT + \" Analyzing sentiment...\"\n",
    "                        polarity_classifier = PolarityClassifier(self.pos_tagger, self.lexicon, debug=DEBUG)\n",
    "                        sentiment, score, nscore = polarity_classifier.classify(sentence)\n",
    "                        if DEBUG: print Tcolors.RES + Tcolors.OKGREEN + \" \" + sentiment + Tcolors.ENDC\n",
    "                    # If sentence is objective\n",
    "                    elif result == 'objective':\n",
    "                        sentiment = 'neutral'  \n",
    "                    \n",
    "                    # Collect high-confidence training instances for SVM classifier.\n",
    "                    # After the training, SVM can be used to classify new sentences.\n",
    "                    #if sentiment != \"neutral\" and sentiment != \"\": \n",
    "                        #if sentiment != \"neutral\" and abs(nscore) >= 0.4:\n",
    "                        #   self.total_sentences.append(sentence)\n",
    "                        #   self.total_sentiments.append(sentiment)\n",
    "                        \n",
    "                    # Store results to memory\n",
    "                    sentiments.append(sentiment)\n",
    "                    scores.append(score)\n",
    "                    nscores.append(nscore)\n",
    "                    \n",
    "                    # Update score\n",
    "                    if results.has_key(sentiment):\n",
    "                        results[sentiment]['nscore'] += nscore\n",
    "                        results[sentiment]['score'] += score\n",
    "                        results[sentiment]['count'] += 1 \n",
    "                          \n",
    "                print       \n",
    "                print Tcolors.ACT + \" Overall sentiment analysis:\"\n",
    "                print Tcolors.BGH\n",
    "                print \" Parts: \", len(sentences)\n",
    "                print \" Sentiments: \", sentiments\n",
    "                print \" Scores: \", scores \n",
    "                print \" Results: \", \"},\\n\\t    \".join((str)(results).split(\"}, \"))\n",
    "                print Tcolors.C\n",
    "\n",
    "                pcount = results['positive']['count']\n",
    "                ncount = results['negative']['count'] \n",
    "                total = len(sentences)\n",
    "                print Tcolors.BG\n",
    "                print \" subjective\".ljust(16,\"-\") + \"> %.2f\" % ((float)(pcount + ncount)*100 / total) + \"%\"\n",
    "                print \" objective\".ljust(16,\"-\") + \"> %.2f\" % (100 - ((float)(pcount + ncount)*100 / total)) + \"%\"\n",
    "                print Tcolors.C\n",
    "                print Tcolors.BGGRAY\n",
    "                for sense in results.keys():\n",
    "                    count = results[sense]['count']\n",
    "                    percentage = (float)(count) * 100 / (len(sentences))\n",
    "                    print \" \" +sense.ljust(15,\"-\")+\"> %.2f\" % (percentage) + \"%\"\n",
    "                  \n",
    "                print Tcolors.C \n",
    "                ssum = sum(scores)\n",
    "                confidence = \" (%.2f, %.2f)\" % (ssum,sum(nscores))\n",
    "                final_sent = \"\"\n",
    "                pos = True\n",
    "                if results[\"negative\"][\"count\"] > len(sentences)*1.0/3:\n",
    "                    pos = False\n",
    "\n",
    "                # Print total sentiment score and normalized sentiment score\n",
    "                if ssum > 0 and pos:\n",
    "                    print Tcolors.RES + Tcolors.OKGREEN + \" positive\" + confidence + Tcolors.C\n",
    "                    final_sent = \"positive\"\n",
    "                elif ssum == 0:\n",
    "                    print Tcolors.RES + Tcolors.OKGREEN +  \" neutral\" + confidence + Tcolors.C\n",
    "                    final_sent = \"neutral\"\n",
    "                else:\n",
    "                    print Tcolors.RES + Tcolors.OKGREEN +  \" negative\" + confidence + Tcolors.C\n",
    "                    final_sent = \"negative\"\n",
    "                print Tcolors.C\n",
    "                \n",
    "                # Store results\n",
    "                total_result_hash = {'sentences' : sentences,\n",
    "                                     'sentiments': sentiments,\n",
    "                                     'scores'    : scores,\n",
    "                                     'nscores'   : nscores,\n",
    "                                     'results'   : results,\n",
    "                                      'final' : {final_sent:{'score':ssum,'nscore':sum(nscores)}}} \n",
    "        # Train SVM classifier\n",
    "        # self.train_svm()\n",
    "        return total_result_hash\n",
    "    \n",
    "    def normalize(self, text):\n",
    "        \"\"\"\n",
    "            Make some word improvements before feeding to the sentence tokenizer.\n",
    "        \"\"\"  \n",
    "        rr = RepeatReplacer(self.lexicon)\n",
    "        normalized_text = []\n",
    "        final = None\n",
    "        try:\n",
    "            for word in text.split():\n",
    "                normal = rr.replace(word.lower()) \n",
    "                if word[0].isupper(): \n",
    "                    normal = normal[0].upper() + normal[1:]\n",
    "                \n",
    "                normalized_text.append(normal)\n",
    "                final = \" \".join(normalized_text)\n",
    "        except:\n",
    "                final = text\n",
    "    \n",
    "        return final\n",
    "                \n",
    "    def train_svm(self):\n",
    "        \"\"\"\n",
    "            Train SVM and store data with pickle.\n",
    "        \"\"\"\n",
    "        self.svm.train(self.total_sentences, self.total_sentiments)\n",
    "        t_output = open(self.svm_train_filename,'wb')\n",
    "        l_output = open(self.svm_label_filename,'wb')\n",
    "        pickle.dump(self.total_sentences,t_output)\n",
    "        pickle.dump(self.total_sentiments,l_output)\n",
    "        t_output.close()\n",
    "        l_output.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':       \n",
    "    sentiment = Sentiment()\n",
    "    if len(sys.argv) > 1:\n",
    "        sentiment.analyze([sys.argv[1]]) \n",
    "    else:\n",
    "        sentiment.analyze([u\"I was blown away by some of the comments here posted by people who is either uneducated, ignorant, self-righteous or all-of-the-above...I'm irritated and saddened as I read these \\\"finger-pointing\\\" or \\\"I'm right and you're wrong\\\" type of posts! Grow up folks! You're not in grade school...learn to embrace what is positive and move forward to do what is right... I have to give much love and respect to Ronny...your work is AMAZING!!! You cannot fathom how good I feel after I watched this video...regardless of history, politics, or whatever forces that makes what the mid-east today...for what you did and many of the followers in Iran and Palestine ...I BELIEVE TOMORROW WILL BE BETTER!!!!!! My name is Christopher Lee, I'm a nurse in Los Angeles and I {HEART} YOU ALL (especially to all of you beautiful and sweet ladies across the way)!!!!!\"])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named terminal_colors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-fbe121c8e72c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mterminal_colors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTcolors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpb_classifiers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPbSubj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named terminal_colors"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys  \n",
    "import pickle \n",
    "from terminal_colors import Tcolors\n",
    "from pb_classifiers import PbSubj\n",
    "\n",
    "class Bootstrapping:\n",
    "    \"\"\"\n",
    "        Bootstrapping: Class performing the bootstrapping process for \n",
    "        subjectivity and objectivity classification of  sentences. The \n",
    "        method learns linguistically rich extraction patterns for subjective \n",
    "        (opinionated) expressions from unannotated data. The learned\n",
    "        patterns are used to identify more subjective sentences that simple \n",
    "        high precision classifiers can't recall.\n",
    "        Related paper:\n",
    "        E. Riloff and J. Wiebe. Learning extraction patterns for subjective \n",
    "        expressions. In Proceedings of the 2003 conference on Empirical methods \n",
    "        in natural language processing, EMNLP '03, pages 105--112, 2003. ACL.\n",
    "        \n",
    "        Learned patterns structure\n",
    "        e.g. {\"<subj> was killed\" : {'type' : 'subj',\n",
    "                                     'display' : 'was killed',\n",
    "                                     'subj_freq' : 10,\n",
    "                                     'freq' : 20,\n",
    "                                     'prob' : 0.5}}\n",
    "         \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hp_obj, hp_subj, tagger, debug=False):\n",
    "        # Syntactic forms for pattern extraction\n",
    "        self.syntactic_forms = {\"subj\" : [[\"BE\",\"VBN*|VBD*\"],\n",
    "                                          [\"HAVE\",\"BE\",\"VB*\"],\n",
    "                                          [\"VB*\"],\n",
    "                                          [\"VB*\",\"*\",\"NN*|NP*|NC*\"], \n",
    "                                          [\"VB*\",\"TO\",\"VB*\"],\n",
    "                                          [\"HAVE\",\"TO\",\"BE\"],\n",
    "                                          [\"HAVE\",\"NN*\"]],\n",
    "                                \"dobj\" : [[\"VB*\"],\n",
    "                                          [\"TO\",\"VB*\"],\n",
    "                                          [\"VB*\",\"TO\",\"VB*\"]],  \n",
    "                                \"np\"   : [[\"NN\",\"IN\"],\n",
    "                                          [\"VB*\",\"NN\",\"IN\"],\n",
    "                                          [\"BE\",\"VBN\",\"IN\"],\n",
    "                                          [\"TO\",\"VB\",\"TO\"]]\n",
    "                                }\n",
    "        self.filename = \"stored/learned_patterns\"\n",
    "        try:\n",
    "            self.learned_patterns = pickle.load(open(self.filename))\n",
    "            print Tcolors.ADD + Tcolors.OKBLUE + \" Loaded existing pattern knowledge!\" + Tcolors.ENDC \n",
    "        except:\n",
    "            print Tcolors.ACT + Tcolors.RED + \" Existing pattern knowledge not found.\" + Tcolors.ENDC\n",
    "            self.learned_patterns = {}\n",
    "             \n",
    "        # Part Of Speech Sequential Tagger (Unigram->Bigram->Trigram) \n",
    "        self.tagger = tagger\n",
    "        # Sentence to be classified\n",
    "        self.subjective = False\n",
    "        self.objective = False\n",
    "        # High precision objective classifier\n",
    "        self.hp_obj = hp_obj\n",
    "        # High precision subjective classifier\n",
    "        self.hp_subj = hp_subj\n",
    "        # Pattern-Based Subjective Classifier\n",
    "        self.pb_subj = PbSubj(self.tagger, debug=debug)\n",
    "        # Learned patterns\n",
    "        self.patterns = {}\n",
    "        self.debug = debug\n",
    "            \n",
    "    def classify(self, sentence, previous=\"\", next=\"\"):\n",
    "        \"\"\"\n",
    "            Subjectivity classification using boostrapping method.\n",
    "        \"\"\"\n",
    "        # STEP 1: Classify sentence with HP Subjective classifier\n",
    "        self.subjective = self.hp_subj.classify(sentence) \n",
    "        # STEP 1: Get help from learned patterns\n",
    "        if not self.subjective:\n",
    "            if self.debug: print Tcolors.ACT + \" Training pattern based classifier...\\n\"\n",
    "            self.pb_subj.train(self.learned_patterns)\n",
    "            found, self.subjective, obj = self.pb_subj.classify(sentence)\n",
    "        \n",
    "        if not self.subjective and not self.objective:\n",
    "            # STEP 2: Classify sentence with HP Objective classifier\n",
    "            self.objective = self.hp_obj.classify(sentence, previous, next)\n",
    "        \n",
    "        if self.subjective or self.objective:\n",
    "            # STEP 3: Learn \n",
    "            self.learn_patterns_from(sentence) \n",
    "        else:\n",
    "            # STEP 4: Classify based on learned patterns\n",
    "            found, self.subjective, self.objective = self.pb_subj.classify(sentence)\n",
    "            # Uncomment the two following to bootstrap further the subjective\n",
    "            # sentences detected from the pattern-based classifier.\n",
    "            # if self.subjective:\n",
    "            #    self.learn_patterns_from(sentence)\n",
    "        if self.subjective:\n",
    "            return 'subjective'\n",
    "        elif self.objective:\n",
    "            return 'objective'\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def learn_patterns_from(self, sentence):\n",
    "        \"\"\"\n",
    "            Learns extraction patterns associated with subjectivity\n",
    "            from a given sentence.\n",
    "        \"\"\"\n",
    "        tagged_sentence = self.tagger.tag(sentence)\n",
    "        tags = []\n",
    "        words = []\n",
    "        if self.debug:\n",
    "            print Tcolors.ACT + \" Performing part of speech (POS) tagging...\" + Tcolors.WARNING \n",
    "            print tagged_sentence\n",
    "            print Tcolors.ENDC\n",
    "        for (w,tag) in tagged_sentence:\n",
    "            if tag is None:\n",
    "                tag = \"\"\n",
    "            tags.append(tag)\n",
    "            words.append(w)\n",
    "\n",
    "        self.trigger_patterns(tags, words)\n",
    "                \n",
    "    def match_until_next_nn(self, i, tags, words, form, key):\n",
    "        \"\"\"\n",
    "            The hard job for triggering the syntactic forms :-)\n",
    "        \"\"\"\n",
    "        LIMITER = 4\n",
    "        BE = ['was','were','be','being','am','been','are','is']\n",
    "        HAVE = ['has','have','had']\n",
    "        matched = 0\n",
    "        prev_matched = 0 \n",
    "        positions_matched = []\n",
    "        learned_pattern = []\n",
    "        star = False \n",
    "\n",
    "        for j,ctag in enumerate(form):\n",
    "            next = i + j + 1\n",
    "            inner = 0 \n",
    "            found = False\n",
    "            while(not found and next < len(tags)):\n",
    "                next += inner\n",
    "                if next < len(words) and ctag == \"VB*\" and words[next] in HAVE:\n",
    "                    next += 1\n",
    "                    if next < len(words) and ctag == \"VB*\" and words[next] in BE:\n",
    "                        next += 1\n",
    "                elif next < len(words) and ctag == \"VB*\" and words[next] in BE:\n",
    "                    next += 1\n",
    "                if ctag == \"*\":\n",
    "                    star = True  \n",
    "                elif ctag.find(\"*\") > -1:\n",
    "                    ortags = ctag.split(\"|\")\n",
    "                    for ortag in ortags:\n",
    "                        if next < len(tags) and tags[next].find(ortag.replace(\"*\",\"\")) > -1\\\n",
    "                           and next not in positions_matched: \n",
    "                            if star and inner < 2: \n",
    "                                matched += 1\n",
    "                            matched += 1 \n",
    "                            positions_matched.append(next) \n",
    "                            found = True\n",
    "                elif ctag == \"BE\":  \n",
    "                    if next < len(tags) and (tags[next].find(\"VB\") > -1 or tags[next].find(\"BE\") > -1) \\\n",
    "                       and words[next] in BE and next not in positions_matched: \n",
    "                        matched += 1 \n",
    "                        positions_matched.append(next)\n",
    "                        found = True\n",
    "                elif ctag == \"HAVE\":\n",
    "                    if next < len(tags) and (tags[next].find(\"VB\") > -1 or tags[next].find(\"HV\") > -1)\\\n",
    "                       and words[next] in HAVE and next not in positions_matched:\n",
    "                            matched += 1 \n",
    "                            positions_matched.append(next)  \n",
    "                            found = True                 \n",
    "                elif next < len(tags) and tags[next].find(ctag) > -1\\\n",
    "                     and next not in positions_matched: \n",
    "                    matched += 1\n",
    "                    positions_matched.append(next)\n",
    "                    found = True\n",
    "                else:\n",
    "                    found = True\n",
    "                inner += 1\n",
    "                \n",
    "        if key == \"subj\":\n",
    "            learned_pattern = [\"<subj>\"] \n",
    "        for pos in positions_matched:\n",
    "            learned_pattern.append(words[pos])\n",
    "        if key != \"subj\":\n",
    "            learned_pattern.append(\"<\" + key +\">\")\n",
    "        \n",
    "        learned_pattern = \" \".join(learned_pattern)    \n",
    "         \n",
    "        if matched == len(form):\n",
    "            if self.debug:\n",
    "                print Tcolors.ACT + Tcolors.RED + \" Form triggered: \", form, Tcolors.ENDC\n",
    "                print \"Pattern learned:\", learned_pattern\n",
    "            return True, learned_pattern\n",
    "        else:\n",
    "            return False, None\n",
    "                \n",
    "    def proccess_learned_pattern(self, pattern):\n",
    "        \"\"\"\n",
    "            Add pattern to learned patterns if it doesn't exist else\n",
    "            update its probability.\n",
    "        \"\"\" \n",
    "        if pattern.find(\"subj\") > -1:\n",
    "            key = \"subj\"\n",
    "        elif pattern.find(\"dobj\") > -1:\n",
    "            key = \"dobj\"\n",
    "        else:\n",
    "            key = \"np\"\n",
    "        cur_subj_freq = 0\n",
    "        if self.subjective:\n",
    "            cur_subj_freq = 1 \n",
    "        pkey = pattern\n",
    "        pkey = re.sub(r\"<subj> | <np>| <dobj>\",\"\",pkey) \n",
    "        if self.learned_patterns.has_key(pattern):\n",
    "            subj_freq = self.learned_patterns[pattern]['subj_freq'] + cur_subj_freq\n",
    "            freq = self.learned_patterns[pattern]['freq'] + 1\n",
    "            prob = (float)(subj_freq)/(float)(freq)\n",
    "            self.learned_patterns[pattern]['prob'] = prob\n",
    "            self.learned_patterns[pattern]['subj_freq'] = subj_freq\n",
    "            self.learned_patterns[pattern]['freq'] = freq\n",
    "            if self.debug: print Tcolors.ADD + Tcolors.HEADER + \" Updating pattern:\", pattern, Tcolors.ENDC  \n",
    "        else:\n",
    "            subj_freq = 0\n",
    "            freq = 1\n",
    "            subj_freq += cur_subj_freq\n",
    "            prob = (float)(subj_freq)/(float)(freq)\n",
    "            self.learned_patterns[pattern] = {'type': key,\n",
    "                                           'display': pkey,\n",
    "                                           'freq' : freq,\n",
    "                                           'subj_freq' : subj_freq,\n",
    "                                           'prob' : prob}  \n",
    "            if self.debug: print Tcolors.ADD + Tcolors.CYAN + \" Learning pattern:\", pattern, Tcolors.ENDC  \n",
    "            \n",
    "    def store_knowledge(self): \n",
    "        \"\"\"\n",
    "            Stored learned patterns for future usage.\n",
    "        \"\"\"\n",
    "        output = open(self.filename, 'wb')\n",
    "        pickle.dump(self.learned_patterns, output)\n",
    "        \n",
    "        \n",
    "    def trigger_patterns(self, tags, words):\n",
    "        \"\"\"\n",
    "            Method that triggers syntactic forms and returns the learned \n",
    "            patterns from the triggering.\n",
    "        \"\"\"   \n",
    "        patterns = []\n",
    "        if self.debug: print Tcolors.ACT + \" Triggering subjective syntactic forms...\" \n",
    "        for key in self.syntactic_forms.keys():\n",
    "            syntactic_forms = self.syntactic_forms[key]\n",
    "            if self.debug: print Tcolors.PROC + Tcolors.GRAY + \" Checking form group \" + key + \"...\" + Tcolors.ENDC\n",
    "            \n",
    "            for form in syntactic_forms:  \n",
    "                for i,tag in enumerate(tags): \n",
    "                    if tag.find(\"NN\") > -1 or tag.find(\"NP\") > -1 \\\n",
    "                       or tag.find(\"PR\") > -1: \n",
    "                        triggered, pattern = self.match_until_next_nn(i, tags, words, form, key) \n",
    "                        if pattern is not None and pattern not in patterns:\n",
    "                            if self.debug: print Tcolors.ACT + Tcolors.RED + \" Form triggered: \", form, Tcolors.ENDC\n",
    "                            patterns.append(pattern)\n",
    "        for pattern in patterns:\n",
    "            self.proccess_learned_pattern(pattern)\n",
    "        if self.debug:\n",
    "            print Tcolors.OKBLUE\n",
    "            print self.learned_patterns    \n",
    "            print Tcolors.ENDC\n",
    "        self.store_knowledge()\n",
    "\n",
    "    def train(self, data):\n",
    "        \"\"\"\n",
    "            Method to train the pattern-based classifier\n",
    "        \"\"\"\n",
    "        for sentence in data:\n",
    "            self.classify(sentence)  \n",
    "    \n",
    "    def clear_learned_data(self):\n",
    "        self.learned_patterns = {}\n",
    "\n",
    "                  \n",
    "if __name__ == \"__main__\":\n",
    "    from hp_classifiers import HpObj, HpSubj\n",
    "    from pos import SequentialTagger\n",
    "    hp_obj = HpObj()\n",
    "    hp_subj = HpSubj()  \n",
    "    tagger = SequentialTagger()\n",
    "    bootstrapping = Bootstrapping(hp_obj, hp_subj, tagger)\n",
    "    if self.debug:\n",
    "        print bootstrapping.classify(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tcolors:\n",
    "    HEADER = '\\033[1;95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[1;92m'\n",
    "    WARNING = '\\033[1;93m'\n",
    "    W = '\\033[1;37m'\n",
    "    GRAY = W\n",
    "    BGGRAY = '\\033[1;37;40m'\n",
    "    BG = '\\033[1;30;47m' \n",
    "    BGH = '\\033[1;40;41m' \n",
    "    FAIL = '\\033[91m'\n",
    "    RED = '\\033[1;91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    CYAN = '\\033[1;36m'\n",
    "    INF = '\\033[1;90m'\n",
    "    C = ENDC \n",
    "    \n",
    "    ACT = W + \"[\"+RED+\"*\"+ENDC+W+\"]\" + C\n",
    "    PROC = W + \"[\"+OKBLUE+\"*\"+ENDC+W+\"]\" + C\n",
    "    ADD = W + \"[\"+WARNING+\"+\"+ENDC+W+\"]\" + C\n",
    "    RES = W + \"[\"+OKGREEN+\"x\"+ENDC+W+\"]\" + C\n",
    "    INFO = W + \"[\"+OKBLUE+\"INFO:\"+ENDC+W+\"]\" + C\n",
    "    OK = W + \"[ \"+ OKGREEN + \"OK\" + ENDC+W+ \" ]\" + C\n",
    "    def disable(self):\n",
    "        self.HEADER = ''\n",
    "        self.OKBLUE = ''\n",
    "        self.OKGREEN = ''\n",
    "        self.WARNING = ''\n",
    "        self.FAIL = ''\n",
    "        self.ENDC = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named PyML",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c00944fbb3b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPyML\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatsel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPyML\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontainers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorDatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparseDataSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVectorDataSet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPyML\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomposite\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFeatureSelect\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named PyML"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PyML import svm, ker, featsel\n",
    "from PyML.containers.vectorDatasets import SparseDataSet, VectorDataSet\n",
    "from PyML.classifiers.composite import Chain, FeatureSelect \n",
    "from scrapy.conf import settings\n",
    "from terminal_colors import Tcolors\n",
    "from PyML.classifiers.svm import loadSVM\n",
    "\n",
    "class SvmClassifier:\n",
    "    \"\"\"\n",
    "    SVM classifier: Performing training and prediction of sentiment class.\n",
    "    \"\"\"\n",
    "    def __init__(self, lexicon, C=1, num_features=100):\n",
    "        self.training_set = None\n",
    "        self.classes = None \n",
    "        self.test_set = None\n",
    "        self.results = None\n",
    "        self.kernel = ker.Linear()\n",
    "        self.C = C  \n",
    "        self.feature_data = PATH + \"/learning/stored/feature.data\"\n",
    "        self.label_data = PATH + \"/learning/stored/svm_label.data\"\n",
    "        self.lexicon = lexicon\n",
    "        self.num_features = len(self.lexicon.words.keys())\n",
    "        try:\n",
    "            print \"Loading existing SVM...\"\n",
    "            features = pickle.load(open(self.feature_data))\n",
    "            labels = pickle.load(open(self.label_data))\n",
    "            sparsedata = SparseDataSet(features, L=labels) \n",
    "            self.svm_classifier = loadSVM(PATH + \"/learning/stored/svm.classifier\",sparsedata)\n",
    "        except Exception as e:\n",
    "            print e\n",
    "            print \"Existing SVM not found!\"\n",
    "            self.svm_classifier = svm.SVM(self.kernel)\n",
    "        self.accuracy = None\n",
    "        self.predicted_labels = None\n",
    "        score = featsel.FeatureScore('golub')\n",
    "        self.filter = featsel.Filter(score)\n",
    "        self.feature_selector = FeatureSelect(self.svm_classifier, self.filter)\n",
    "        self.chain = Chain([self.feature_selector, self.svm_classifier])\n",
    "        \n",
    "    def classify(self, sentences, labels):\n",
    "        self.test_set = self.compute_features(sentences)\n",
    "        print\n",
    "        print Tcolors.ACT + \" Classifying instance with SVM: \" + Tcolors.RED + sentences[0] + Tcolors.C\n",
    "        print Tcolors.HEADER\n",
    "        test_data = SparseDataSet(self.test_set, L=labels)\n",
    "        self.results = self.svm_classifier.test(test_data)\n",
    "        print Tcolors.C \n",
    "        return self.results\n",
    "    \n",
    "    def compute_features(self, sentences): \n",
    "        features = [] \n",
    "        for i,sent in enumerate(sentences):\n",
    "            sent = sent.lower()\n",
    "            words = nltk.word_tokenize(sent)     \n",
    "            feature = np.zeros(self.num_features) \n",
    "            for word in words:\n",
    "                if word.lower() in self.lexicon.words.keys():\n",
    "                    feature[self.lexicon.words.keys().index(word)] = 1\n",
    "            features.append(feature) \n",
    "        return features  \n",
    "    \n",
    "    def initialize_lexicon(self):\n",
    "        pass\n",
    "    \n",
    "    def print_stats(self): \n",
    "        print \"[*] SVM Classifier ACCURACY: \", self.accuracy\n",
    "        print \"[*] SVM Classifier PREDICTED_LABEL: \", self.predicted_labels[0]\n",
    "    \n",
    "    def stats(self):\n",
    "        self.accuracy = self.results.getSuccessRate()\n",
    "        self.predicted_labels = self.results.getPredictedLabels()        \n",
    "    \n",
    "    def save(self,data,features,labels):\n",
    "        output = open(self.feature_data ,'wb')\n",
    "        pickle.dump(features,output)\n",
    "        output.close()\n",
    "        output = open(self.label_data,'wb')\n",
    "        pickle.dump(labels,output)\n",
    "        output.close()\n",
    "        self.svm_classifier.save(PATH + \"/learning/stored/svm.classifier\")\n",
    "        \n",
    "    def train(self, training_set, labels):\n",
    "        print Tcolors.ACT + \" Training SVM with chaining...\"\n",
    "        features = self.compute_features(training_set) \n",
    "        data = SparseDataSet(features, L=labels) \n",
    "        print Tcolors.CYAN\n",
    "        self.training_set = data \n",
    "        self.svm_classifier.train(data)     \n",
    "        self.save(data,features,labels)\n",
    "        print Tcolors.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named PyML",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c00944fbb3b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPyML\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatsel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPyML\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontainers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorDatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparseDataSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVectorDataSet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPyML\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomposite\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFeatureSelect\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named PyML"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PyML import svm, ker, featsel\n",
    "from PyML.containers.vectorDatasets import SparseDataSet, VectorDataSet\n",
    "from PyML.classifiers.composite import Chain, FeatureSelect \n",
    "from scrapy.conf import settings\n",
    "from terminal_colors import Tcolors\n",
    "from PyML.classifiers.svm import loadSVM\n",
    "\n",
    "class SvmClassifier:\n",
    "    \"\"\"\n",
    "    SVM classifier: Performing training and prediction of sentiment class.\n",
    "    \"\"\"\n",
    "    def __init__(self, lexicon, C=1, num_features=100):\n",
    "        self.training_set = None\n",
    "        self.classes = None \n",
    "        self.test_set = None\n",
    "        self.results = None\n",
    "        self.kernel = ker.Linear()\n",
    "        self.C = C  \n",
    "        self.feature_data = PATH + \"/learning/stored/feature.data\"\n",
    "        self.label_data = PATH + \"/learning/stored/svm_label.data\"\n",
    "        self.lexicon = lexicon\n",
    "        self.num_features = len(self.lexicon.words.keys())\n",
    "        try:\n",
    "            print \"Loading existing SVM...\"\n",
    "            features = pickle.load(open(self.feature_data))\n",
    "            labels = pickle.load(open(self.label_data))\n",
    "            sparsedata = SparseDataSet(features, L=labels) \n",
    "            self.svm_classifier = loadSVM(PATH + \"/learning/stored/svm.classifier\",sparsedata)\n",
    "        except Exception as e:\n",
    "            print e\n",
    "            print \"Existing SVM not found!\"\n",
    "            self.svm_classifier = svm.SVM(self.kernel)\n",
    "        self.accuracy = None\n",
    "        self.predicted_labels = None\n",
    "        score = featsel.FeatureScore('golub')\n",
    "        self.filter = featsel.Filter(score)\n",
    "        self.feature_selector = FeatureSelect(self.svm_classifier, self.filter)\n",
    "        self.chain = Chain([self.feature_selector, self.svm_classifier])\n",
    "        \n",
    "    def classify(self, sentences, labels):\n",
    "        self.test_set = self.compute_features(sentences)\n",
    "        print\n",
    "        print Tcolors.ACT + \" Classifying instance with SVM: \" + Tcolors.RED + sentences[0] + Tcolors.C\n",
    "        print Tcolors.HEADER\n",
    "        test_data = SparseDataSet(self.test_set, L=labels)\n",
    "        self.results = self.svm_classifier.test(test_data)\n",
    "        print Tcolors.C \n",
    "        return self.results\n",
    "    \n",
    "    def compute_features(self, sentences): \n",
    "        features = [] \n",
    "        for i,sent in enumerate(sentences):\n",
    "            sent = sent.lower()\n",
    "            words = nltk.word_tokenize(sent)     \n",
    "            feature = np.zeros(self.num_features) \n",
    "            for word in words:\n",
    "                if word.lower() in self.lexicon.words.keys():\n",
    "                    feature[self.lexicon.words.keys().index(word)] = 1\n",
    "            features.append(feature) \n",
    "        return features  \n",
    "    \n",
    "    def initialize_lexicon(self):\n",
    "        pass\n",
    "    \n",
    "    def print_stats(self): \n",
    "        print \"[*] SVM Classifier ACCURACY: \", self.accuracy\n",
    "        print \"[*] SVM Classifier PREDICTED_LABEL: \", self.predicted_labels[0]\n",
    "    \n",
    "    def stats(self):\n",
    "        self.accuracy = self.results.getSuccessRate()\n",
    "        self.predicted_labels = self.results.getPredictedLabels()        \n",
    "    \n",
    "    def save(self,data,features,labels):\n",
    "        output = open(self.feature_data ,'wb')\n",
    "        pickle.dump(features,output)\n",
    "        output.close()\n",
    "        output = open(self.label_data,'wb')\n",
    "        pickle.dump(labels,output)\n",
    "        output.close()\n",
    "        self.svm_classifier.save(PATH + \"/learning/stored/svm.classifier\")\n",
    "        \n",
    "    def train(self, training_set, labels):\n",
    "        print Tcolors.ACT + \" Training SVM with chaining...\"\n",
    "        features = self.compute_features(training_set) \n",
    "        data = SparseDataSet(features, L=labels) \n",
    "        print Tcolors.CYAN\n",
    "        self.training_set = data \n",
    "        self.svm_classifier.train(data)     \n",
    "        self.save(data,features,labels)\n",
    "        print Tcolors.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named lexicon",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b89d05a6a59e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlexicon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLexicon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstemming\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mporter2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named lexicon"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import nltk\n",
    "from lexicon import Lexicon    \n",
    "from stemming.porter2 import stem\n",
    "\n",
    "class HpSubj:\n",
    "    \"\"\"\n",
    "        High precision subjective sentence classifier which uses an annotated \n",
    "        lexicon of words as features. It classifies a sentence as subjective \n",
    "        if it contains two or more of the strong subjective clues.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, debug=False): \n",
    "        self.dictionary = Lexicon().words\n",
    "        self.debug = debug\n",
    "        \n",
    "    def classify(self, sentence):\n",
    "        wdict = self.dictionary\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        strong_subjective_words_count = 0\n",
    "        subjective = False\n",
    "        for word in words: \n",
    "            word = word.lower()\n",
    "            check = [word, stem(word)]\n",
    "            for w in check: \n",
    "                if wdict.has_key(w) and wdict[w]['type'] == 'strongsubj':\n",
    "                    strong_subjective_words_count += 1\n",
    "                    if strong_subjective_words_count >= 2:\n",
    "                        subjective = True\n",
    "                        break\n",
    "        return subjective\n",
    "    \n",
    "\n",
    "class HpObj:\n",
    "    \"\"\"\n",
    "        High precision objective sentence classifier which uses an annotated \n",
    "        lexicon as training data. It classifies a sentence as objective if it\n",
    "        doesn't contain along with its previous and next sentence, not even \n",
    "        one strong subjective clue and at most one weak subjective clue.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, debug=False): \n",
    "    \n",
    "        self.lexicon = Lexicon()\n",
    "        self.dictionary = self.lexicon.words\n",
    "        self.debug = debug\n",
    "    \n",
    "    def classify(self, current, previous=\"\", next=\"\"):\n",
    "        if self.debug:\n",
    "            print \n",
    "            print \"current:\", current\n",
    "            print \"previous:\", previous \n",
    "            print \"next:\", next\n",
    "            print\n",
    "        wdict = self.dictionary\n",
    "        words = nltk.word_tokenize(current)\n",
    "        prev_words = nltk.word_tokenize(previous)\n",
    "        next_words = nltk.word_tokenize(next)\n",
    "        words += prev_words + next_words\n",
    "        strong_subjective_words_count = 0\n",
    "        weak_subjective_words_count = 0 \n",
    "        objective = True\n",
    "        \n",
    "        for word in words: \n",
    "            word = word.lower()\n",
    "            check = [word, stem(word)]\n",
    "            for w in check: \n",
    "                if wdict.has_key(w):\n",
    "                    if wdict[w]['type'] == 'strongsubj':\n",
    "                        strong_subjective_words_count += 1\n",
    "                        if strong_subjective_words_count > 0:\n",
    "                            objective = False\n",
    "                            break\n",
    "                    elif wdict[w]['type'] == 'weaksubj':\n",
    "                        weak_subjective_words_count += 1\n",
    "                        if weak_subjective_words_count > 1:\n",
    "                            objective = False\n",
    "                            break\n",
    "        return objective\n",
    " \n",
    "\n",
    "if __name__ == '__main__': \n",
    "    hpo = HpObj()\n",
    "    hps = HpSubj()\n",
    "    print \"Objective: \" + (str)(hpo.classify(sys.argv[1]))\n",
    "    print \"Subjective: \" + (str)(hps.classify(sys.argv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named datasets.emoticons_patch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0ec2d23ea5e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memoticons_patch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpatch_emoticons\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mLexicon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named datasets.emoticons_patch"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import os\n",
    "import sys \n",
    "from datasets.emoticons_patch import patch_emoticons \n",
    "\n",
    "class Lexicon:\n",
    "    \"\"\"\n",
    "        Lexicon class loads an annotated dataset of words\n",
    "        that have strong/weak subjectivity and is used to \n",
    "        train the high precision objective and subjective \n",
    "        classifiers.\n",
    "    \"\"\"\n",
    "       \n",
    "    def __init__(self):\n",
    "        self.filename =  \"stored/lexicon\"\n",
    "        try:\n",
    "            self.words = pickle.load(open(self.filename))\n",
    "        except:\n",
    "            self.words = {}\n",
    "            self.load()\n",
    "            output = open(self.filename, 'wb')\n",
    "            pickle.dump(self.words, output)\n",
    "        \n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "            The method loads the annotated corpus and extracts the structure\n",
    "            with easy access for the classifiers.\n",
    "        \"\"\"\n",
    "        dictionary_file = open(\"datasets/subjclueslen1-HLTEMNLP05.tff\",\"r\")\n",
    "        lines = dictionary_file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            attributes = line.split(\" \")\n",
    "            for index,attr in enumerate(attributes):\n",
    "                if attr.find('word1') > -1:\n",
    "                    word_value = attr.split(\"=\")[1]\n",
    "                    attributes[index] = []\n",
    "                    break\n",
    "            if self.words.has_key(word_value):\n",
    "                for attr in attributes:\n",
    "                    if attr != []:\n",
    "                        arr = attr.split(\"=\")\n",
    "                        key = arr[0]\n",
    "                        if key == \"pos1\":  \n",
    "                            pos = self.words[word_value][key]\n",
    "                            self.words[word_value][key].append(arr[1])\n",
    "                            break\n",
    "            else:\n",
    "                self.words[word_value] = {}\n",
    "                for attr in attributes:\n",
    "                    if attr != []:\n",
    "                        arr = attr.split(\"=\")\n",
    "                        key = arr[0]\n",
    "                        if len(arr) > 1:\n",
    "                            value = arr[1]\n",
    "                        if key == \"pos1\":\n",
    "                            self.words[word_value][key] = [value.replace(\"\\n\", \"\")]\n",
    "                        else:\n",
    "                            self.words[word_value][key] = value.replace(\"\\n\", \"\") \n",
    "        \n",
    "        self.words = dict(patch_emoticons(), **self.words)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
